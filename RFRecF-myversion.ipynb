{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(nn.Module):\n",
    "    def __init__(self, lambda_u, lambda_L, prob=0.5):\n",
    "        super(Loss, self).__init__()\n",
    "        self.lambda_u = lambda_u\n",
    "        self.lambda_L = lambda_L\n",
    "        self.prob = prob\n",
    "\n",
    "    def compute_f_loss(self, rating_mat, user_features, local_item_features):\n",
    "        non_zero_mask = (rating_mat != -1).type(torch.FloatTensor)\n",
    "        ratings_predicted = torch.sigmoid(torch.mm(user_features, local_item_features.t()))\n",
    "        \n",
    "        diff = (ratings_predicted - rating_mat) ** 2\n",
    "        prediction_error = torch.sum(diff * non_zero_mask)\n",
    "\n",
    "        user_regularization = torch.sum(user_features ** 2)  ## regularization term for user features\n",
    "\n",
    "        return (prediction_error + self.lambda_u * user_regularization) / (1 - self.prob)\n",
    "    \n",
    "    def compute_psi_loss(self, local_item_features, avg_item_features):\n",
    "        item_loss = torch.sum((local_item_features - avg_item_features) ** 2) ## loss term for item features\n",
    "\n",
    "        return self.lambda_L * item_loss / self.prob\n",
    "\n",
    "    def forward(self, rating_mat, user_features, local_item_features, avg_item_features):\n",
    "        '''\n",
    "        rating_mat: (num_users, num_items)\n",
    "        user_features: (num_users_per_client, num_latent_factors)\n",
    "        local_item_features: (num_items, num_latent_factors)\n",
    "        avg_item_features: (num_items, num_latent_factors)\n",
    "        '''\n",
    "        non_zero_mask = (rating_mat != -1).type(torch.FloatTensor)\n",
    "        ratings_predicted = torch.sigmoid(torch.mm(user_features, local_item_features.t()))\n",
    "        \n",
    "        diff = (ratings_predicted - rating_mat) ** 2\n",
    "        prediction_error = torch.sum(diff * non_zero_mask)\n",
    "\n",
    "        user_regularization = torch.sum(user_features ** 2)  ## regularization term for user features\n",
    "        item_loss = torch.sum((local_item_features - avg_item_features) ** 2) ## loss term for item features\n",
    "\n",
    "        loss = prediction_error + self.lambda_u * user_regularization + self.lambda_L * item_loss\n",
    "\n",
    "        return loss, prediction_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>916931</th>\n",
       "      <td>5539</td>\n",
       "      <td>107</td>\n",
       "      <td>3</td>\n",
       "      <td>960662169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46608</th>\n",
       "      <td>312</td>\n",
       "      <td>3617</td>\n",
       "      <td>3</td>\n",
       "      <td>976477183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504652</th>\n",
       "      <td>3104</td>\n",
       "      <td>1617</td>\n",
       "      <td>5</td>\n",
       "      <td>969556952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443640</th>\n",
       "      <td>2736</td>\n",
       "      <td>1244</td>\n",
       "      <td>2</td>\n",
       "      <td>973396870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410451</th>\n",
       "      <td>2462</td>\n",
       "      <td>1196</td>\n",
       "      <td>4</td>\n",
       "      <td>974168782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id  item_id  rating  timestamp\n",
       "916931     5539      107       3  960662169\n",
       "46608       312     3617       3  976477183\n",
       "504652     3104     1617       5  969556952\n",
       "443640     2736     1244       2  973396870\n",
       "410451     2462     1196       4  974168782"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_df = pd.read_csv('ml-1m.inter', sep='\\t')\n",
    "rating_df.columns = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "rating_df = shuffle(rating_df)\n",
    "rating_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "ratio = 0.8\n",
    "train_size = int(len(rating_df) * ratio)\n",
    "\n",
    "aggregate_rating_matrix = rating_df.pivot_table(index='user_id', columns='item_id', values='rating', aggfunc='mean')  # transform the dataframe into a matrix\n",
    "num_users, num_items = aggregate_rating_matrix.shape\n",
    "rating_matrix = aggregate_rating_matrix.copy()\n",
    "test_rating_matrix = aggregate_rating_matrix.copy()\n",
    "for i in range(len(rating_df)):\n",
    "    user_id = rating_df.iloc[i,0]\n",
    "    item_id = rating_df.iloc[i,1]\n",
    "    if i < train_size:\n",
    "        test_rating_matrix.loc[user_id,item_id] = None\n",
    "    else:\n",
    "        rating_matrix.loc[user_id,item_id] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the ratings using min-max normalization\n",
    "min_rating, max_rating = rating_df['rating'].min(), rating_df['rating'].max()\n",
    "rating_matrix = rating_matrix.apply(lambda x: (x - min_rating) / (max_rating - min_rating))\n",
    "rating_matrix[rating_matrix.isnull()] = -1\n",
    "rating_matrix = torch.FloatTensor(rating_matrix.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rating_matrix[test_rating_matrix.isnull()] = -1\n",
    "test_rating_matrix = torch.FloatTensor(test_rating_matrix.values)\n",
    "print(test_rating_matrix.shape)\n",
    "\n",
    "nonzero_mask = (test_rating_matrix != -1).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rating_matrix.pkl', 'rb') as f:\n",
    "    rating_matrix = pickle.load(f)\n",
    "with open('test_rating_matrix.pkl', 'rb') as f:\n",
    "    test_rating_matrix = pickle.load(f)\n",
    "\n",
    "nonzero_mask = (test_rating_matrix != -1).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6040, 3706)"
      ]
     },
     "execution_count": 814,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_users, num_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 815,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "\n",
    "lr = 0.05\n",
    "lambda_u = 0.1\n",
    "lambda_L = 10\n",
    "num_epochs = 200\n",
    "latent_factors = 20\n",
    "num_clients = 200\n",
    "prob_threshold = 0.5\n",
    "m = num_users // num_clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 816,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializaiton\n",
    "\n",
    "user_features = []\n",
    "item_features = []\n",
    "std = 0.01\n",
    "\n",
    "for i in range(num_clients): # initialize user features and local item features\n",
    "    user_features.append(torch.randn(m, latent_factors, requires_grad=True))  # multiplying std here will make the Tensor non-leaf, which will cause error\n",
    "    item_features.append(torch.randn(num_items, latent_factors, requires_grad=True))\n",
    "with torch.no_grad():\n",
    "    for i in range(num_clients):\n",
    "        user_features[i].data.mul_(std) # mul_ does not change requires_grad to False\n",
    "        item_features[i].data.mul_(std)\n",
    "\n",
    "avg_item_features = torch.randn(num_items, latent_factors).data.mul(std) # mul will change requires_grad to False\n",
    "for i in range(num_clients):\n",
    "    avg_item_features += item_features[i]\n",
    "avg_item_features /= num_clients\n",
    "\n",
    "# define the model\n",
    "RFRecF_loss = Loss(lambda_u=lambda_u, lambda_L=lambda_L, prob=prob_threshold)\n",
    "\n",
    "client_optimizers = []\n",
    "for i in range(num_clients):\n",
    "    optimizer = optim.Adam([user_features[i], item_features[i]], lr=lr)\n",
    "    client_optimizers.append(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, rand_num, last_num):\n",
    "    avg_loss = 0\n",
    "    global avg_item_features\n",
    "\n",
    "    # update\n",
    "    if rand_num > prob_threshold or epoch == 0:\n",
    "        for i in range(num_clients):\n",
    "            client_optimizers[i].zero_grad()\n",
    "            if last_num > prob_threshold or epoch == 0:\n",
    "                loss = RFRecF_loss.compute_f_loss(rating_matrix[i*m: (i+1)*m], user_features[i], item_features[i])\n",
    "                avg_loss += loss.item() / num_clients\n",
    "            else:\n",
    "                loss = RFRecF_loss.compute_psi_loss(item_features[i], avg_item_features)\n",
    "                \n",
    "            loss.backward(retain_graph=True)\n",
    "            client_optimizers[i].step()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            avg_item_features = sum(item_features) / num_clients  # update the global item features\n",
    "\n",
    "    print('Epoch: {}, Loss: {:.4f}, '.format(epoch, avg_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 787.9926, \n",
      "Epoch: 1, Loss: 0.0000, \n",
      "Epoch: 2, Loss: 0.0000, \n",
      "Epoch: 3, Loss: 0.0000, \n",
      "Epoch: 4, Loss: 0.0000, \n",
      "Epoch: 5, Loss: 789.2048, \n",
      "Epoch: 6, Loss: 0.0000, \n",
      "Epoch: 7, Loss: 0.0000, \n",
      "Epoch: 8, Loss: 0.0000, \n",
      "Epoch: 9, Loss: 0.0000, \n",
      "Epoch: 10, Loss: 0.0000, \n",
      "Epoch: 11, Loss: 0.0000, \n",
      "Epoch: 12, Loss: 0.0000, \n",
      "Epoch: 13, Loss: 0.0000, \n",
      "Epoch: 14, Loss: 792.6638, \n",
      "Epoch: 15, Loss: 802.2932, \n",
      "Epoch: 16, Loss: 801.1841, \n",
      "Epoch: 17, Loss: 785.5033, \n",
      "Epoch: 18, Loss: 756.6974, \n",
      "Epoch: 19, Loss: 0.0000, \n",
      "Epoch: 20, Loss: 0.0000, \n",
      "Epoch: 21, Loss: 0.0000, \n",
      "Epoch: 22, Loss: 0.0000, \n",
      "Epoch: 23, Loss: 0.0000, \n",
      "Epoch: 24, Loss: 0.0000, \n",
      "Epoch: 25, Loss: 0.0000, \n",
      "Epoch: 26, Loss: 0.0000, \n",
      "Epoch: 27, Loss: 0.0000, \n",
      "Epoch: 28, Loss: 0.0000, \n",
      "Epoch: 29, Loss: 0.0000, \n",
      "Epoch: 30, Loss: 758.7466, \n",
      "Epoch: 31, Loss: 773.2893, \n",
      "Epoch: 32, Loss: 803.2160, \n",
      "Epoch: 33, Loss: 0.0000, \n",
      "Epoch: 34, Loss: 0.0000, \n",
      "Epoch: 35, Loss: 0.0000, \n",
      "Epoch: 36, Loss: 0.0000, \n",
      "Epoch: 37, Loss: 0.0000, \n",
      "Epoch: 38, Loss: 0.0000, \n",
      "Epoch: 39, Loss: 0.0000, \n",
      "Epoch: 40, Loss: 0.0000, \n",
      "Epoch: 41, Loss: 880.1598, \n",
      "Epoch: 42, Loss: 0.0000, \n",
      "Epoch: 43, Loss: 0.0000, \n",
      "Epoch: 44, Loss: 0.0000, \n",
      "Epoch: 45, Loss: 0.0000, \n",
      "Epoch: 46, Loss: 0.0000, \n",
      "Epoch: 47, Loss: 0.0000, \n",
      "Epoch: 48, Loss: 0.0000, \n",
      "Epoch: 49, Loss: 0.0000, \n",
      "Epoch: 50, Loss: 0.0000, \n",
      "Epoch: 51, Loss: 0.0000, \n",
      "Epoch: 52, Loss: 0.0000, \n",
      "Epoch: 53, Loss: 0.0000, \n",
      "Epoch: 54, Loss: 0.0000, \n",
      "Epoch: 55, Loss: 0.0000, \n",
      "Epoch: 56, Loss: 0.0000, \n",
      "Epoch: 57, Loss: 0.0000, \n",
      "Epoch: 58, Loss: 748.3325, \n",
      "Epoch: 59, Loss: 721.0033, \n",
      "Epoch: 60, Loss: 690.7189, \n",
      "Epoch: 61, Loss: 656.9612, \n",
      "Epoch: 62, Loss: 0.0000, \n",
      "Epoch: 63, Loss: 0.0000, \n",
      "Epoch: 64, Loss: 0.0000, \n",
      "Epoch: 65, Loss: 0.0000, \n",
      "Epoch: 66, Loss: 0.0000, \n",
      "Epoch: 67, Loss: 0.0000, \n",
      "Epoch: 68, Loss: 0.0000, \n",
      "Epoch: 69, Loss: 644.5362, \n",
      "Epoch: 70, Loss: 0.0000, \n",
      "Epoch: 71, Loss: 0.0000, \n",
      "Epoch: 72, Loss: 0.0000, \n",
      "Epoch: 73, Loss: 0.0000, \n",
      "Epoch: 74, Loss: 0.0000, \n",
      "Epoch: 75, Loss: 0.0000, \n",
      "Epoch: 76, Loss: 694.5403, \n",
      "Epoch: 77, Loss: 725.7222, \n",
      "Epoch: 78, Loss: 0.0000, \n",
      "Epoch: 79, Loss: 0.0000, \n",
      "Epoch: 80, Loss: 0.0000, \n",
      "Epoch: 81, Loss: 0.0000, \n",
      "Epoch: 82, Loss: 0.0000, \n",
      "Epoch: 83, Loss: 0.0000, \n",
      "Epoch: 84, Loss: 0.0000, \n",
      "Epoch: 85, Loss: 0.0000, \n",
      "Epoch: 86, Loss: 0.0000, \n",
      "Epoch: 87, Loss: 0.0000, \n",
      "Epoch: 88, Loss: 0.0000, \n",
      "Epoch: 89, Loss: 0.0000, \n",
      "Epoch: 90, Loss: 0.0000, \n",
      "Epoch: 91, Loss: 0.0000, \n",
      "Epoch: 92, Loss: 0.0000, \n",
      "Epoch: 93, Loss: 0.0000, \n",
      "Epoch: 94, Loss: 0.0000, \n",
      "Epoch: 95, Loss: 0.0000, \n",
      "Epoch: 96, Loss: 0.0000, \n",
      "Epoch: 97, Loss: 0.0000, \n",
      "Epoch: 98, Loss: 0.0000, \n",
      "Epoch: 99, Loss: 0.0000, \n",
      "Epoch: 100, Loss: 0.0000, \n",
      "Epoch: 101, Loss: 0.0000, \n",
      "Epoch: 102, Loss: 0.0000, \n",
      "Epoch: 103, Loss: 0.0000, \n",
      "Epoch: 104, Loss: 0.0000, \n",
      "Epoch: 105, Loss: 0.0000, \n",
      "Epoch: 106, Loss: 0.0000, \n",
      "Epoch: 107, Loss: 0.0000, \n",
      "Epoch: 108, Loss: 0.0000, \n",
      "Epoch: 109, Loss: 770.4966, \n",
      "Epoch: 110, Loss: 742.0255, \n",
      "Epoch: 111, Loss: 707.3881, \n",
      "Epoch: 112, Loss: 666.9776, \n",
      "Epoch: 113, Loss: 0.0000, \n",
      "Epoch: 114, Loss: 0.0000, \n",
      "Epoch: 115, Loss: 0.0000, \n",
      "Epoch: 116, Loss: 0.0000, \n",
      "Epoch: 117, Loss: 0.0000, \n",
      "Epoch: 118, Loss: 0.0000, \n",
      "Epoch: 119, Loss: 0.0000, \n",
      "Epoch: 120, Loss: 0.0000, \n",
      "Epoch: 121, Loss: 680.3878, \n",
      "Epoch: 122, Loss: 713.1641, \n",
      "Epoch: 123, Loss: 737.6296, \n",
      "Epoch: 124, Loss: 0.0000, \n",
      "Epoch: 125, Loss: 0.0000, \n",
      "Epoch: 126, Loss: 0.0000, \n",
      "Epoch: 127, Loss: 0.0000, \n",
      "Epoch: 128, Loss: 0.0000, \n",
      "Epoch: 129, Loss: 773.6279, \n",
      "Epoch: 130, Loss: 0.0000, \n",
      "Epoch: 131, Loss: 0.0000, \n",
      "Epoch: 132, Loss: 0.0000, \n",
      "Epoch: 133, Loss: 0.0000, \n",
      "Epoch: 134, Loss: 0.0000, \n",
      "Epoch: 135, Loss: 0.0000, \n",
      "Epoch: 136, Loss: 0.0000, \n",
      "Epoch: 137, Loss: 795.8650, \n",
      "Epoch: 138, Loss: 786.1665, \n",
      "Epoch: 139, Loss: 761.1471, \n",
      "Epoch: 140, Loss: 0.0000, \n",
      "Epoch: 141, Loss: 0.0000, \n",
      "Epoch: 142, Loss: 0.0000, \n",
      "Epoch: 143, Loss: 0.0000, \n",
      "Epoch: 144, Loss: 0.0000, \n",
      "Epoch: 145, Loss: 693.2448, \n",
      "Epoch: 146, Loss: 666.3405, \n",
      "Epoch: 147, Loss: 626.6258, \n",
      "Epoch: 148, Loss: 576.7222, \n",
      "Epoch: 149, Loss: 520.2554, \n",
      "Epoch: 150, Loss: 0.0000, \n",
      "Epoch: 151, Loss: 0.0000, \n",
      "Epoch: 152, Loss: 0.0000, \n",
      "Epoch: 153, Loss: 0.0000, \n",
      "Epoch: 154, Loss: 0.0000, \n",
      "Epoch: 155, Loss: 0.0000, \n",
      "Epoch: 156, Loss: 0.0000, \n",
      "Epoch: 157, Loss: 0.0000, \n",
      "Epoch: 158, Loss: 0.0000, \n",
      "Epoch: 159, Loss: 0.0000, \n",
      "Epoch: 160, Loss: 637.9655, \n",
      "Epoch: 161, Loss: 0.0000, \n",
      "Epoch: 162, Loss: 0.0000, \n",
      "Epoch: 163, Loss: 0.0000, \n",
      "Epoch: 164, Loss: 0.0000, \n",
      "Epoch: 165, Loss: 0.0000, \n",
      "Epoch: 166, Loss: 0.0000, \n",
      "Epoch: 167, Loss: 800.7487, \n",
      "Epoch: 168, Loss: 800.8377, \n",
      "Epoch: 169, Loss: 0.0000, \n",
      "Epoch: 170, Loss: 0.0000, \n",
      "Epoch: 171, Loss: 0.0000, \n",
      "Epoch: 172, Loss: 730.2383, \n",
      "Epoch: 173, Loss: 660.4692, \n",
      "Epoch: 174, Loss: 0.0000, \n",
      "Epoch: 175, Loss: 0.0000, \n",
      "Epoch: 176, Loss: 0.0000, \n",
      "Epoch: 177, Loss: 0.0000, \n",
      "Epoch: 178, Loss: 0.0000, \n",
      "Epoch: 179, Loss: 0.0000, \n",
      "Epoch: 180, Loss: 0.0000, \n",
      "Epoch: 181, Loss: 0.0000, \n",
      "Epoch: 182, Loss: 0.0000, \n",
      "Epoch: 183, Loss: 502.9534, \n",
      "Epoch: 184, Loss: 461.1478, \n",
      "Epoch: 185, Loss: 0.0000, \n",
      "Epoch: 186, Loss: 0.0000, \n",
      "Epoch: 187, Loss: 412.7801, \n",
      "Epoch: 188, Loss: 396.9996, \n",
      "Epoch: 189, Loss: 376.9620, \n",
      "Epoch: 190, Loss: 353.8225, \n",
      "Epoch: 191, Loss: 328.9378, \n",
      "Epoch: 192, Loss: 0.0000, \n",
      "Epoch: 193, Loss: 0.0000, \n",
      "Epoch: 194, Loss: 0.0000, \n",
      "Epoch: 195, Loss: 0.0000, \n",
      "Epoch: 196, Loss: 0.0000, \n",
      "Epoch: 197, Loss: 0.0000, \n",
      "Epoch: 198, Loss: 0.0000, \n",
      "Epoch: 199, Loss: 429.2610, \n"
     ]
    }
   ],
   "source": [
    "last_num = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    rand_num = np.random.rand()\n",
    "    train(epoch, rand_num, last_num)\n",
    "    last_num = rand_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(matrix, user_features, item_features, mask):\n",
    "    predicted_ratings = torch.sigmoid(torch.mm(user_features, item_features.t()))\n",
    "    pred = (predicted_ratings * (max_rating - min_rating) + min_rating) * mask\n",
    "    true_value = matrix * mask\n",
    "    \n",
    "    abs_error = torch.sum(torch.abs(pred - true_value))\n",
    "    square_error = torch.sum((pred - true_value)**2)\n",
    "    n_nonzero = torch.sum(mask)\n",
    "    return abs_error, square_error, n_nonzero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  0.7839408\n",
      "RMSE:  0.98345464\n"
     ]
    }
   ],
   "source": [
    "MAE = MSE = num_nonzero = 0\n",
    "\n",
    "for i in range(num_clients):\n",
    "    abs_error, square_error, n_nonzero = evaluate(test_rating_matrix[i*m: (i+1)*m], user_features[i], item_features[i], nonzero_mask[i*m: (i+1)*m])\n",
    "    MAE += abs_error\n",
    "    MSE += square_error\n",
    "    num_nonzero += n_nonzero\n",
    "\n",
    "MAE /= num_nonzero\n",
    "RMSE = torch.sqrt(MSE / num_nonzero)\n",
    "print(\"MAE: \", MAE.data.numpy())\n",
    "print(\"RMSE: \", RMSE.data.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ordinary",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
