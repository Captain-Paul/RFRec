{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pickle\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Loss(nn.Module):\n",
    "    def __init__(self, lambda_u, lambda_L, prob=0.5):\n",
    "        super(Loss, self).__init__()\n",
    "        self.lambda_u = lambda_u\n",
    "        self.lambda_L = lambda_L\n",
    "        self.prob = prob\n",
    "\n",
    "    def compute_f_loss(self, rating_mat, user_features, local_item_features):\n",
    "        non_zero_mask = (rating_mat != -1).type(torch.FloatTensor)\n",
    "        ratings_predicted = torch.sigmoid(torch.mm(user_features, local_item_features.t()))\n",
    "        \n",
    "        diff = (ratings_predicted - rating_mat) ** 2\n",
    "        prediction_error = torch.sum(diff * non_zero_mask)\n",
    "\n",
    "        user_regularization = torch.sum(user_features ** 2)  ## regularization term for user features\n",
    "\n",
    "        return (prediction_error + self.lambda_u * user_regularization)\n",
    "    \n",
    "    def compute_psi_loss(self, local_item_features, avg_item_features):\n",
    "        item_loss = torch.sum((local_item_features - avg_item_features) ** 2) ## loss term for item features\n",
    "\n",
    "        return self.lambda_L * item_loss\n",
    "\n",
    "    def forward(self, rating_mat, user_features, local_item_features, avg_item_features):\n",
    "        '''\n",
    "        rating_mat: (num_users, num_items)\n",
    "        user_features: (num_users_per_client, num_latent_factors)\n",
    "        local_item_features: (num_items, num_latent_factors)\n",
    "        avg_item_features: (num_items, num_latent_factors)\n",
    "        '''\n",
    "        non_zero_mask = (rating_mat != -1).type(torch.FloatTensor)\n",
    "        ratings_predicted = torch.sigmoid(torch.mm(user_features, local_item_features.t()))\n",
    "        \n",
    "        diff = (ratings_predicted - rating_mat) ** 2\n",
    "        prediction_error = torch.sum(diff * non_zero_mask)\n",
    "\n",
    "        user_regularization = torch.sum(user_features ** 2)  ## regularization term for user features\n",
    "        item_loss = torch.sum((local_item_features - avg_item_features) ** 2) ## loss term for item features\n",
    "\n",
    "        loss = prediction_error + self.lambda_u * user_regularization + self.lambda_L * item_loss\n",
    "\n",
    "        return loss, prediction_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>916931</th>\n",
       "      <td>5539</td>\n",
       "      <td>107</td>\n",
       "      <td>3</td>\n",
       "      <td>960662169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46608</th>\n",
       "      <td>312</td>\n",
       "      <td>3617</td>\n",
       "      <td>3</td>\n",
       "      <td>976477183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>504652</th>\n",
       "      <td>3104</td>\n",
       "      <td>1617</td>\n",
       "      <td>5</td>\n",
       "      <td>969556952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>443640</th>\n",
       "      <td>2736</td>\n",
       "      <td>1244</td>\n",
       "      <td>2</td>\n",
       "      <td>973396870</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>410451</th>\n",
       "      <td>2462</td>\n",
       "      <td>1196</td>\n",
       "      <td>4</td>\n",
       "      <td>974168782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id  item_id  rating  timestamp\n",
       "916931     5539      107       3  960662169\n",
       "46608       312     3617       3  976477183\n",
       "504652     3104     1617       5  969556952\n",
       "443640     2736     1244       2  973396870\n",
       "410451     2462     1196       4  974168782"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rating_df = pd.read_csv('ml-1m.inter', sep='\\t')\n",
    "rating_df.columns = ['user_id', 'item_id', 'rating', 'timestamp']\n",
    "rating_df = shuffle(rating_df)\n",
    "rating_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and testing sets\n",
    "ratio = 0.8\n",
    "train_size = int(len(rating_df) * ratio)\n",
    "\n",
    "aggregate_rating_matrix = rating_df.pivot_table(index='user_id', columns='item_id', values='rating', aggfunc='mean')  # transform the dataframe into a matrix\n",
    "num_users, num_items = aggregate_rating_matrix.shape\n",
    "rating_matrix = aggregate_rating_matrix.copy()\n",
    "test_rating_matrix = aggregate_rating_matrix.copy()\n",
    "for i in range(len(rating_df)):\n",
    "    user_id = rating_df.iloc[i,0]\n",
    "    item_id = rating_df.iloc[i,1]\n",
    "    if i < train_size:\n",
    "        test_rating_matrix.loc[user_id,item_id] = None\n",
    "    else:\n",
    "        rating_matrix.loc[user_id,item_id] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the ratings using min-max normalization\n",
    "min_rating, max_rating = rating_df['rating'].min(), rating_df['rating'].max()\n",
    "rating_matrix = rating_matrix.apply(lambda x: (x - min_rating) / (max_rating - min_rating))\n",
    "rating_matrix[rating_matrix.isnull()] = -1\n",
    "rating_matrix = torch.FloatTensor(rating_matrix.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_rating_matrix[test_rating_matrix.isnull()] = -1\n",
    "test_rating_matrix = torch.FloatTensor(test_rating_matrix.values)\n",
    "print(test_rating_matrix.shape)\n",
    "\n",
    "nonzero_mask = (test_rating_matrix != -1).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rating_matrix.pkl', 'rb') as f:\n",
    "    rating_matrix = pickle.load(f)\n",
    "with open('test_rating_matrix.pkl', 'rb') as f:\n",
    "    test_rating_matrix = pickle.load(f)\n",
    "\n",
    "nonzero_mask = (test_rating_matrix != -1).type(torch.FloatTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6040, 3706)"
      ]
     },
     "execution_count": 628,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_users, num_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 0.1\n",
    "lambda_u = 0.1\n",
    "lambda_L = 20\n",
    "num_epochs = 500\n",
    "latent_factors = 20\n",
    "num_clients = 200\n",
    "prob_threshold = 0.5\n",
    "m = num_users // num_clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initializaiton\n",
    "\n",
    "user_features = []\n",
    "item_features = []\n",
    "std = 0.01\n",
    "\n",
    "for i in range(num_clients): # initialize user features and local item features\n",
    "    user_features.append(torch.randn(m, latent_factors, requires_grad=True))  # multiplying std here will make the Tensor non-leaf, which will cause error\n",
    "    item_features.append(torch.randn(num_items, latent_factors, requires_grad=True))\n",
    "with torch.no_grad():\n",
    "    for i in range(num_clients):\n",
    "        user_features[i].data.mul_(std) # mul_ does not change requires_grad to False\n",
    "        item_features[i].data.mul_(std)\n",
    "\n",
    "avg_item_features = torch.randn(num_items, latent_factors).data.mul(std) # mul will change requires_grad to False\n",
    "for i in range(num_clients):\n",
    "    avg_item_features += item_features[i]\n",
    "avg_item_features /= num_clients\n",
    "\n",
    "# define the model\n",
    "RFRecF_loss = Loss(lambda_u=lambda_u, lambda_L=lambda_L, prob=prob_threshold)\n",
    "\n",
    "client_optimizers = []\n",
    "for i in range(num_clients):\n",
    "    optimizer = optim.Adam([user_features[i], item_features[i]], lr=lr)\n",
    "    client_optimizers.append(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, rand_num, last_num):\n",
    "    avg_loss = 0\n",
    "    global avg_item_features\n",
    "\n",
    "    # update\n",
    "    if rand_num > prob_threshold or epoch == 0:\n",
    "        for i in range(num_clients):\n",
    "            client_optimizers[i].zero_grad()\n",
    "            if last_num > prob_threshold or epoch == 0:\n",
    "                loss = RFRecF_loss.compute_f_loss(rating_matrix[i*m: (i+1)*m], user_features[i], item_features[i])\n",
    "                avg_loss += loss.item() / num_clients\n",
    "            else:\n",
    "                loss = RFRecF_loss.compute_psi_loss(item_features[i], avg_item_features)\n",
    "            loss.backward(retain_graph=True)\n",
    "            client_optimizers[i].step()\n",
    "    else:\n",
    "        with torch.no_grad():\n",
    "            avg_item_features = sum(item_features) / num_clients  # update the global item features\n",
    "\n",
    "    print('Epoch: {}, Loss: {:.4f}, '.format(epoch, avg_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 393.9960, \n",
      "Epoch: 1, Loss: 0.0000, \n",
      "Epoch: 2, Loss: 0.0000, \n",
      "Epoch: 3, Loss: 0.0000, \n",
      "Epoch: 4, Loss: 0.0000, \n",
      "Epoch: 5, Loss: 0.0000, \n",
      "Epoch: 6, Loss: 0.0000, \n",
      "Epoch: 7, Loss: 0.0000, \n",
      "Epoch: 8, Loss: 0.0000, \n",
      "Epoch: 9, Loss: 0.0000, \n",
      "Epoch: 10, Loss: 0.0000, \n",
      "Epoch: 11, Loss: 396.6311, \n",
      "Epoch: 12, Loss: 0.0000, \n",
      "Epoch: 13, Loss: 0.0000, \n",
      "Epoch: 14, Loss: 0.0000, \n",
      "Epoch: 15, Loss: 0.0000, \n",
      "Epoch: 16, Loss: 0.0000, \n",
      "Epoch: 17, Loss: 384.0932, \n",
      "Epoch: 18, Loss: 389.2525, \n",
      "Epoch: 19, Loss: 409.1574, \n",
      "Epoch: 20, Loss: 0.0000, \n",
      "Epoch: 21, Loss: 0.0000, \n",
      "Epoch: 22, Loss: 0.0000, \n",
      "Epoch: 23, Loss: 0.0000, \n",
      "Epoch: 24, Loss: 0.0000, \n",
      "Epoch: 25, Loss: 0.0000, \n",
      "Epoch: 26, Loss: 0.0000, \n",
      "Epoch: 27, Loss: 0.0000, \n",
      "Epoch: 28, Loss: 0.0000, \n",
      "Epoch: 29, Loss: 0.0000, \n",
      "Epoch: 30, Loss: 396.0177, \n",
      "Epoch: 31, Loss: 374.2104, \n",
      "Epoch: 32, Loss: 349.3487, \n",
      "Epoch: 33, Loss: 321.3566, \n",
      "Epoch: 34, Loss: 290.3416, \n",
      "Epoch: 35, Loss: 0.0000, \n",
      "Epoch: 36, Loss: 0.0000, \n",
      "Epoch: 37, Loss: 0.0000, \n",
      "Epoch: 38, Loss: 0.0000, \n",
      "Epoch: 39, Loss: 0.0000, \n",
      "Epoch: 40, Loss: 0.0000, \n",
      "Epoch: 41, Loss: 0.0000, \n",
      "Epoch: 42, Loss: 311.3178, \n",
      "Epoch: 43, Loss: 0.0000, \n",
      "Epoch: 44, Loss: 0.0000, \n",
      "Epoch: 45, Loss: 418.3431, \n",
      "Epoch: 46, Loss: 492.8383, \n",
      "Epoch: 47, Loss: 563.4907, \n",
      "Epoch: 48, Loss: 614.0896, \n",
      "Epoch: 49, Loss: 0.0000, \n",
      "Epoch: 50, Loss: 0.0000, \n",
      "Epoch: 51, Loss: 0.0000, \n",
      "Epoch: 52, Loss: 0.0000, \n",
      "Epoch: 53, Loss: 528.4435, \n",
      "Epoch: 54, Loss: 0.0000, \n",
      "Epoch: 55, Loss: 0.0000, \n",
      "Epoch: 56, Loss: 0.0000, \n",
      "Epoch: 57, Loss: 0.0000, \n",
      "Epoch: 58, Loss: 0.0000, \n",
      "Epoch: 59, Loss: 0.0000, \n",
      "Epoch: 60, Loss: 0.0000, \n",
      "Epoch: 61, Loss: 0.0000, \n",
      "Epoch: 62, Loss: 325.0763, \n",
      "Epoch: 63, Loss: 310.4964, \n",
      "Epoch: 64, Loss: 0.0000, \n",
      "Epoch: 65, Loss: 0.0000, \n",
      "Epoch: 66, Loss: 300.9572, \n",
      "Epoch: 67, Loss: 303.5609, \n",
      "Epoch: 68, Loss: 301.0455, \n",
      "Epoch: 69, Loss: 294.1361, \n",
      "Epoch: 70, Loss: 0.0000, \n",
      "Epoch: 71, Loss: 0.0000, \n",
      "Epoch: 72, Loss: 0.0000, \n",
      "Epoch: 73, Loss: 0.0000, \n",
      "Epoch: 74, Loss: 0.0000, \n",
      "Epoch: 75, Loss: 0.0000, \n",
      "Epoch: 76, Loss: 0.0000, \n",
      "Epoch: 77, Loss: 394.0305, \n",
      "Epoch: 78, Loss: 0.0000, \n",
      "Epoch: 79, Loss: 0.0000, \n",
      "Epoch: 80, Loss: 0.0000, \n",
      "Epoch: 81, Loss: 0.0000, \n",
      "Epoch: 82, Loss: 480.3688, \n",
      "Epoch: 83, Loss: 0.0000, \n",
      "Epoch: 84, Loss: 0.0000, \n",
      "Epoch: 85, Loss: 0.0000, \n",
      "Epoch: 86, Loss: 0.0000, \n",
      "Epoch: 87, Loss: 0.0000, \n",
      "Epoch: 88, Loss: 0.0000, \n",
      "Epoch: 89, Loss: 0.0000, \n",
      "Epoch: 90, Loss: 0.0000, \n",
      "Epoch: 91, Loss: 0.0000, \n",
      "Epoch: 92, Loss: 0.0000, \n",
      "Epoch: 93, Loss: 0.0000, \n",
      "Epoch: 94, Loss: 0.0000, \n",
      "Epoch: 95, Loss: 0.0000, \n",
      "Epoch: 96, Loss: 0.0000, \n",
      "Epoch: 97, Loss: 0.0000, \n",
      "Epoch: 98, Loss: 0.0000, \n",
      "Epoch: 99, Loss: 0.0000, \n",
      "Epoch: 100, Loss: 356.0949, \n",
      "Epoch: 101, Loss: 0.0000, \n",
      "Epoch: 102, Loss: 0.0000, \n",
      "Epoch: 103, Loss: 316.6166, \n",
      "Epoch: 104, Loss: 302.4619, \n",
      "Epoch: 105, Loss: 284.9292, \n",
      "Epoch: 106, Loss: 0.0000, \n",
      "Epoch: 107, Loss: 0.0000, \n",
      "Epoch: 108, Loss: 0.0000, \n",
      "Epoch: 109, Loss: 0.0000, \n",
      "Epoch: 110, Loss: 0.0000, \n",
      "Epoch: 111, Loss: 0.0000, \n",
      "Epoch: 112, Loss: 0.0000, \n",
      "Epoch: 113, Loss: 0.0000, \n",
      "Epoch: 114, Loss: 0.0000, \n",
      "Epoch: 115, Loss: 0.0000, \n",
      "Epoch: 116, Loss: 408.0559, \n",
      "Epoch: 117, Loss: 453.9024, \n",
      "Epoch: 118, Loss: 486.0415, \n",
      "Epoch: 119, Loss: 499.4681, \n",
      "Epoch: 120, Loss: 492.8183, \n",
      "Epoch: 121, Loss: 468.3993, \n",
      "Epoch: 122, Loss: 0.0000, \n",
      "Epoch: 123, Loss: 0.0000, \n",
      "Epoch: 124, Loss: 0.0000, \n",
      "Epoch: 125, Loss: 0.0000, \n",
      "Epoch: 126, Loss: 0.0000, \n",
      "Epoch: 127, Loss: 400.2790, \n",
      "Epoch: 128, Loss: 350.0539, \n",
      "Epoch: 129, Loss: 0.0000, \n",
      "Epoch: 130, Loss: 0.0000, \n",
      "Epoch: 131, Loss: 0.0000, \n",
      "Epoch: 132, Loss: 0.0000, \n",
      "Epoch: 133, Loss: 284.4863, \n",
      "Epoch: 134, Loss: 0.0000, \n",
      "Epoch: 135, Loss: 0.0000, \n",
      "Epoch: 136, Loss: 0.0000, \n",
      "Epoch: 137, Loss: 0.0000, \n",
      "Epoch: 138, Loss: 277.5577, \n",
      "Epoch: 139, Loss: 280.4562, \n",
      "Epoch: 140, Loss: 0.0000, \n",
      "Epoch: 141, Loss: 0.0000, \n",
      "Epoch: 142, Loss: 0.0000, \n",
      "Epoch: 143, Loss: 278.8537, \n",
      "Epoch: 144, Loss: 0.0000, \n",
      "Epoch: 145, Loss: 0.0000, \n",
      "Epoch: 146, Loss: 0.0000, \n",
      "Epoch: 147, Loss: 0.0000, \n",
      "Epoch: 148, Loss: 0.0000, \n",
      "Epoch: 149, Loss: 0.0000, \n",
      "Epoch: 150, Loss: 0.0000, \n",
      "Epoch: 151, Loss: 264.8407, \n",
      "Epoch: 152, Loss: 259.9015, \n",
      "Epoch: 153, Loss: 252.4186, \n",
      "Epoch: 154, Loss: 243.2959, \n",
      "Epoch: 155, Loss: 0.0000, \n",
      "Epoch: 156, Loss: 0.0000, \n",
      "Epoch: 157, Loss: 0.0000, \n",
      "Epoch: 158, Loss: 0.0000, \n",
      "Epoch: 159, Loss: 0.0000, \n",
      "Epoch: 160, Loss: 0.0000, \n",
      "Epoch: 161, Loss: 0.0000, \n",
      "Epoch: 162, Loss: 0.0000, \n",
      "Epoch: 163, Loss: 0.0000, \n",
      "Epoch: 164, Loss: 0.0000, \n",
      "Epoch: 165, Loss: 0.0000, \n",
      "Epoch: 166, Loss: 0.0000, \n",
      "Epoch: 167, Loss: 0.0000, \n",
      "Epoch: 168, Loss: 277.4928, \n",
      "Epoch: 169, Loss: 287.0062, \n",
      "Epoch: 170, Loss: 291.8960, \n",
      "Epoch: 171, Loss: 291.8291, \n",
      "Epoch: 172, Loss: 0.0000, \n",
      "Epoch: 173, Loss: 0.0000, \n",
      "Epoch: 174, Loss: 0.0000, \n",
      "Epoch: 175, Loss: 0.0000, \n",
      "Epoch: 176, Loss: 271.4302, \n",
      "Epoch: 177, Loss: 0.0000, \n",
      "Epoch: 178, Loss: 0.0000, \n",
      "Epoch: 179, Loss: 0.0000, \n",
      "Epoch: 180, Loss: 0.0000, \n",
      "Epoch: 181, Loss: 0.0000, \n",
      "Epoch: 182, Loss: 235.4114, \n",
      "Epoch: 183, Loss: 228.8535, \n",
      "Epoch: 184, Loss: 222.4218, \n",
      "Epoch: 185, Loss: 215.9091, \n",
      "Epoch: 186, Loss: 0.0000, \n",
      "Epoch: 187, Loss: 0.0000, \n",
      "Epoch: 188, Loss: 212.0818, \n",
      "Epoch: 189, Loss: 0.0000, \n",
      "Epoch: 190, Loss: 0.0000, \n",
      "Epoch: 191, Loss: 0.0000, \n",
      "Epoch: 192, Loss: 0.0000, \n",
      "Epoch: 193, Loss: 0.0000, \n",
      "Epoch: 194, Loss: 222.3627, \n",
      "Epoch: 195, Loss: 0.0000, \n",
      "Epoch: 196, Loss: 0.0000, \n",
      "Epoch: 197, Loss: 0.0000, \n",
      "Epoch: 198, Loss: 0.0000, \n",
      "Epoch: 199, Loss: 0.0000, \n",
      "Epoch: 200, Loss: 0.0000, \n",
      "Epoch: 201, Loss: 252.3381, \n",
      "Epoch: 202, Loss: 0.0000, \n",
      "Epoch: 203, Loss: 0.0000, \n",
      "Epoch: 204, Loss: 0.0000, \n",
      "Epoch: 205, Loss: 0.0000, \n",
      "Epoch: 206, Loss: 260.8636, \n",
      "Epoch: 207, Loss: 0.0000, \n",
      "Epoch: 208, Loss: 0.0000, \n",
      "Epoch: 209, Loss: 252.1530, \n",
      "Epoch: 210, Loss: 0.0000, \n",
      "Epoch: 211, Loss: 0.0000, \n",
      "Epoch: 212, Loss: 0.0000, \n",
      "Epoch: 213, Loss: 0.0000, \n",
      "Epoch: 214, Loss: 0.0000, \n",
      "Epoch: 215, Loss: 0.0000, \n",
      "Epoch: 216, Loss: 232.9699, \n",
      "Epoch: 217, Loss: 227.7547, \n",
      "Epoch: 218, Loss: 221.9155, \n",
      "Epoch: 219, Loss: 215.5865, \n",
      "Epoch: 220, Loss: 0.0000, \n",
      "Epoch: 221, Loss: 0.0000, \n",
      "Epoch: 222, Loss: 0.0000, \n",
      "Epoch: 223, Loss: 0.0000, \n",
      "Epoch: 224, Loss: 0.0000, \n",
      "Epoch: 225, Loss: 215.0013, \n",
      "Epoch: 226, Loss: 0.0000, \n",
      "Epoch: 227, Loss: 0.0000, \n",
      "Epoch: 228, Loss: 0.0000, \n",
      "Epoch: 229, Loss: 0.0000, \n",
      "Epoch: 230, Loss: 0.0000, \n",
      "Epoch: 231, Loss: 0.0000, \n",
      "Epoch: 232, Loss: 235.7591, \n",
      "Epoch: 233, Loss: 0.0000, \n",
      "Epoch: 234, Loss: 0.0000, \n",
      "Epoch: 235, Loss: 0.0000, \n",
      "Epoch: 236, Loss: 0.0000, \n",
      "Epoch: 237, Loss: 0.0000, \n",
      "Epoch: 238, Loss: 0.0000, \n",
      "Epoch: 239, Loss: 0.0000, \n",
      "Epoch: 240, Loss: 0.0000, \n",
      "Epoch: 241, Loss: 0.0000, \n",
      "Epoch: 242, Loss: 0.0000, \n",
      "Epoch: 243, Loss: 235.3463, \n",
      "Epoch: 244, Loss: 0.0000, \n",
      "Epoch: 245, Loss: 0.0000, \n",
      "Epoch: 246, Loss: 0.0000, \n",
      "Epoch: 247, Loss: 0.0000, \n",
      "Epoch: 248, Loss: 0.0000, \n",
      "Epoch: 249, Loss: 0.0000, \n",
      "Epoch: 250, Loss: 223.3485, \n",
      "Epoch: 251, Loss: 0.0000, \n",
      "Epoch: 252, Loss: 0.0000, \n",
      "Epoch: 253, Loss: 0.0000, \n",
      "Epoch: 254, Loss: 0.0000, \n",
      "Epoch: 255, Loss: 0.0000, \n",
      "Epoch: 256, Loss: 0.0000, \n",
      "Epoch: 257, Loss: 0.0000, \n",
      "Epoch: 258, Loss: 0.0000, \n",
      "Epoch: 259, Loss: 0.0000, \n",
      "Epoch: 260, Loss: 0.0000, \n",
      "Epoch: 261, Loss: 0.0000, \n",
      "Epoch: 262, Loss: 0.0000, \n",
      "Epoch: 263, Loss: 228.9183, \n",
      "Epoch: 264, Loss: 230.0100, \n",
      "Epoch: 265, Loss: 228.5654, \n",
      "Epoch: 266, Loss: 0.0000, \n",
      "Epoch: 267, Loss: 0.0000, \n",
      "Epoch: 268, Loss: 0.0000, \n",
      "Epoch: 269, Loss: 222.7818, \n",
      "Epoch: 270, Loss: 0.0000, \n",
      "Epoch: 271, Loss: 0.0000, \n",
      "Epoch: 272, Loss: 0.0000, \n",
      "Epoch: 273, Loss: 0.0000, \n",
      "Epoch: 274, Loss: 0.0000, \n",
      "Epoch: 275, Loss: 0.0000, \n",
      "Epoch: 276, Loss: 0.0000, \n",
      "Epoch: 277, Loss: 0.0000, \n",
      "Epoch: 278, Loss: 0.0000, \n",
      "Epoch: 279, Loss: 220.5020, \n",
      "Epoch: 280, Loss: 220.6025, \n",
      "Epoch: 281, Loss: 218.5095, \n",
      "Epoch: 282, Loss: 0.0000, \n",
      "Epoch: 283, Loss: 0.0000, \n",
      "Epoch: 284, Loss: 212.3081, \n",
      "Epoch: 285, Loss: 0.0000, \n",
      "Epoch: 286, Loss: 0.0000, \n",
      "Epoch: 287, Loss: 208.1873, \n",
      "Epoch: 288, Loss: 0.0000, \n",
      "Epoch: 289, Loss: 0.0000, \n",
      "Epoch: 290, Loss: 0.0000, \n",
      "Epoch: 291, Loss: 0.0000, \n",
      "Epoch: 292, Loss: 0.0000, \n",
      "Epoch: 293, Loss: 0.0000, \n",
      "Epoch: 294, Loss: 0.0000, \n",
      "Epoch: 295, Loss: 0.0000, \n",
      "Epoch: 296, Loss: 216.8421, \n",
      "Epoch: 297, Loss: 220.8874, \n",
      "Epoch: 298, Loss: 221.9435, \n",
      "Epoch: 299, Loss: 0.0000, \n",
      "Epoch: 300, Loss: 0.0000, \n",
      "Epoch: 301, Loss: 0.0000, \n",
      "Epoch: 302, Loss: 217.7843, \n",
      "Epoch: 303, Loss: 212.7012, \n",
      "Epoch: 304, Loss: 0.0000, \n",
      "Epoch: 305, Loss: 0.0000, \n",
      "Epoch: 306, Loss: 0.0000, \n",
      "Epoch: 307, Loss: 0.0000, \n",
      "Epoch: 308, Loss: 208.3789, \n",
      "Epoch: 309, Loss: 0.0000, \n",
      "Epoch: 310, Loss: 0.0000, \n",
      "Epoch: 311, Loss: 0.0000, \n",
      "Epoch: 312, Loss: 0.0000, \n",
      "Epoch: 313, Loss: 0.0000, \n",
      "Epoch: 314, Loss: 214.5935, \n",
      "Epoch: 315, Loss: 213.7685, \n",
      "Epoch: 316, Loss: 210.1079, \n",
      "Epoch: 317, Loss: 0.0000, \n",
      "Epoch: 318, Loss: 0.0000, \n",
      "Epoch: 319, Loss: 202.4081, \n",
      "Epoch: 320, Loss: 0.0000, \n",
      "Epoch: 321, Loss: 0.0000, \n",
      "Epoch: 322, Loss: 0.0000, \n",
      "Epoch: 323, Loss: 0.0000, \n",
      "Epoch: 324, Loss: 0.0000, \n",
      "Epoch: 325, Loss: 0.0000, \n",
      "Epoch: 326, Loss: 0.0000, \n",
      "Epoch: 327, Loss: 0.0000, \n",
      "Epoch: 328, Loss: 0.0000, \n",
      "Epoch: 329, Loss: 0.0000, \n",
      "Epoch: 330, Loss: 0.0000, \n",
      "Epoch: 331, Loss: 219.7086, \n",
      "Epoch: 332, Loss: 0.0000, \n",
      "Epoch: 333, Loss: 0.0000, \n",
      "Epoch: 334, Loss: 0.0000, \n",
      "Epoch: 335, Loss: 0.0000, \n",
      "Epoch: 336, Loss: 0.0000, \n",
      "Epoch: 337, Loss: 218.6077, \n",
      "Epoch: 338, Loss: 212.8356, \n",
      "Epoch: 339, Loss: 0.0000, \n",
      "Epoch: 340, Loss: 0.0000, \n",
      "Epoch: 341, Loss: 201.4306, \n",
      "Epoch: 342, Loss: 195.7117, \n",
      "Epoch: 343, Loss: 188.2519, \n",
      "Epoch: 344, Loss: 179.5709, \n",
      "Epoch: 345, Loss: 170.1495, \n",
      "Epoch: 346, Loss: 0.0000, \n",
      "Epoch: 347, Loss: 0.0000, \n",
      "Epoch: 348, Loss: 168.1600, \n",
      "Epoch: 349, Loss: 173.5505, \n",
      "Epoch: 350, Loss: 0.0000, \n",
      "Epoch: 351, Loss: 0.0000, \n",
      "Epoch: 352, Loss: 189.0286, \n",
      "Epoch: 353, Loss: 0.0000, \n",
      "Epoch: 354, Loss: 0.0000, \n",
      "Epoch: 355, Loss: 0.0000, \n",
      "Epoch: 356, Loss: 206.8052, \n",
      "Epoch: 357, Loss: 211.3423, \n",
      "Epoch: 358, Loss: 0.0000, \n",
      "Epoch: 359, Loss: 0.0000, \n",
      "Epoch: 360, Loss: 0.0000, \n",
      "Epoch: 361, Loss: 0.0000, \n",
      "Epoch: 362, Loss: 0.0000, \n",
      "Epoch: 363, Loss: 0.0000, \n",
      "Epoch: 364, Loss: 0.0000, \n",
      "Epoch: 365, Loss: 0.0000, \n",
      "Epoch: 366, Loss: 0.0000, \n",
      "Epoch: 367, Loss: 0.0000, \n",
      "Epoch: 368, Loss: 0.0000, \n",
      "Epoch: 369, Loss: 0.0000, \n",
      "Epoch: 370, Loss: 0.0000, \n",
      "Epoch: 371, Loss: 0.0000, \n",
      "Epoch: 372, Loss: 0.0000, \n",
      "Epoch: 373, Loss: 0.0000, \n",
      "Epoch: 374, Loss: 0.0000, \n",
      "Epoch: 375, Loss: 0.0000, \n",
      "Epoch: 376, Loss: 0.0000, \n",
      "Epoch: 377, Loss: 203.9384, \n",
      "Epoch: 378, Loss: 200.4794, \n",
      "Epoch: 379, Loss: 193.8576, \n",
      "Epoch: 380, Loss: 184.8396, \n",
      "Epoch: 381, Loss: 0.0000, \n",
      "Epoch: 382, Loss: 0.0000, \n",
      "Epoch: 383, Loss: 0.0000, \n",
      "Epoch: 384, Loss: 0.0000, \n",
      "Epoch: 385, Loss: 0.0000, \n",
      "Epoch: 386, Loss: 174.9853, \n",
      "Epoch: 387, Loss: 0.0000, \n",
      "Epoch: 388, Loss: 0.0000, \n",
      "Epoch: 389, Loss: 180.7956, \n",
      "Epoch: 390, Loss: 0.0000, \n",
      "Epoch: 391, Loss: 0.0000, \n",
      "Epoch: 392, Loss: 0.0000, \n",
      "Epoch: 393, Loss: 0.0000, \n",
      "Epoch: 394, Loss: 0.0000, \n",
      "Epoch: 395, Loss: 0.0000, \n",
      "Epoch: 396, Loss: 0.0000, \n",
      "Epoch: 397, Loss: 0.0000, \n",
      "Epoch: 398, Loss: 0.0000, \n",
      "Epoch: 399, Loss: 203.9085, \n",
      "Epoch: 400, Loss: 208.9147, \n",
      "Epoch: 401, Loss: 0.0000, \n",
      "Epoch: 402, Loss: 0.0000, \n",
      "Epoch: 403, Loss: 0.0000, \n",
      "Epoch: 404, Loss: 0.0000, \n",
      "Epoch: 405, Loss: 0.0000, \n",
      "Epoch: 406, Loss: 0.0000, \n",
      "Epoch: 407, Loss: 203.6945, \n",
      "Epoch: 408, Loss: 0.0000, \n",
      "Epoch: 409, Loss: 0.0000, \n",
      "Epoch: 410, Loss: 0.0000, \n",
      "Epoch: 411, Loss: 0.0000, \n",
      "Epoch: 412, Loss: 0.0000, \n",
      "Epoch: 413, Loss: 0.0000, \n",
      "Epoch: 414, Loss: 0.0000, \n",
      "Epoch: 415, Loss: 0.0000, \n",
      "Epoch: 416, Loss: 192.6058, \n",
      "Epoch: 417, Loss: 185.3200, \n",
      "Epoch: 418, Loss: 0.0000, \n",
      "Epoch: 419, Loss: 0.0000, \n",
      "Epoch: 420, Loss: 0.0000, \n",
      "Epoch: 421, Loss: 0.0000, \n",
      "Epoch: 422, Loss: 0.0000, \n",
      "Epoch: 423, Loss: 0.0000, \n",
      "Epoch: 424, Loss: 175.9070, \n",
      "Epoch: 425, Loss: 172.9125, \n",
      "Epoch: 426, Loss: 0.0000, \n",
      "Epoch: 427, Loss: 0.0000, \n",
      "Epoch: 428, Loss: 0.0000, \n",
      "Epoch: 429, Loss: 0.0000, \n",
      "Epoch: 430, Loss: 0.0000, \n",
      "Epoch: 431, Loss: 0.0000, \n",
      "Epoch: 432, Loss: 187.1017, \n",
      "Epoch: 433, Loss: 0.0000, \n",
      "Epoch: 434, Loss: 0.0000, \n",
      "Epoch: 435, Loss: 204.1456, \n",
      "Epoch: 436, Loss: 0.0000, \n",
      "Epoch: 437, Loss: 0.0000, \n",
      "Epoch: 438, Loss: 0.0000, \n",
      "Epoch: 439, Loss: 0.0000, \n",
      "Epoch: 440, Loss: 0.0000, \n",
      "Epoch: 441, Loss: 0.0000, \n",
      "Epoch: 442, Loss: 0.0000, \n",
      "Epoch: 443, Loss: 0.0000, \n",
      "Epoch: 444, Loss: 0.0000, \n",
      "Epoch: 445, Loss: 0.0000, \n",
      "Epoch: 446, Loss: 193.1503, \n",
      "Epoch: 447, Loss: 187.7806, \n",
      "Epoch: 448, Loss: 179.5725, \n",
      "Epoch: 449, Loss: 0.0000, \n",
      "Epoch: 450, Loss: 0.0000, \n",
      "Epoch: 451, Loss: 0.0000, \n",
      "Epoch: 452, Loss: 0.0000, \n",
      "Epoch: 453, Loss: 0.0000, \n",
      "Epoch: 454, Loss: 0.0000, \n",
      "Epoch: 455, Loss: 0.0000, \n",
      "Epoch: 456, Loss: 193.0897, \n",
      "Epoch: 457, Loss: 0.0000, \n",
      "Epoch: 458, Loss: 0.0000, \n",
      "Epoch: 459, Loss: 0.0000, \n",
      "Epoch: 460, Loss: 0.0000, \n",
      "Epoch: 461, Loss: 0.0000, \n",
      "Epoch: 462, Loss: 0.0000, \n",
      "Epoch: 463, Loss: 203.7717, \n",
      "Epoch: 464, Loss: 196.8377, \n",
      "Epoch: 465, Loss: 186.7923, \n",
      "Epoch: 466, Loss: 0.0000, \n",
      "Epoch: 467, Loss: 0.0000, \n",
      "Epoch: 468, Loss: 0.0000, \n",
      "Epoch: 469, Loss: 0.0000, \n",
      "Epoch: 470, Loss: 0.0000, \n",
      "Epoch: 471, Loss: 0.0000, \n",
      "Epoch: 472, Loss: 0.0000, \n",
      "Epoch: 473, Loss: 183.5533, \n",
      "Epoch: 474, Loss: 0.0000, \n",
      "Epoch: 475, Loss: 0.0000, \n",
      "Epoch: 476, Loss: 0.0000, \n",
      "Epoch: 477, Loss: 192.6640, \n",
      "Epoch: 478, Loss: 0.0000, \n",
      "Epoch: 479, Loss: 0.0000, \n",
      "Epoch: 480, Loss: 0.0000, \n",
      "Epoch: 481, Loss: 0.0000, \n",
      "Epoch: 482, Loss: 0.0000, \n",
      "Epoch: 483, Loss: 182.6134, \n",
      "Epoch: 484, Loss: 0.0000, \n",
      "Epoch: 485, Loss: 0.0000, \n",
      "Epoch: 486, Loss: 176.9967, \n",
      "Epoch: 487, Loss: 174.9816, \n",
      "Epoch: 488, Loss: 170.0022, \n",
      "Epoch: 489, Loss: 162.7138, \n",
      "Epoch: 490, Loss: 0.0000, \n",
      "Epoch: 491, Loss: 0.0000, \n",
      "Epoch: 492, Loss: 0.0000, \n",
      "Epoch: 493, Loss: 0.0000, \n",
      "Epoch: 494, Loss: 0.0000, \n",
      "Epoch: 495, Loss: 162.3634, \n",
      "Epoch: 496, Loss: 0.0000, \n",
      "Epoch: 497, Loss: 0.0000, \n",
      "Epoch: 498, Loss: 0.0000, \n",
      "Epoch: 499, Loss: 0.0000, \n"
     ]
    }
   ],
   "source": [
    "last_num = 1\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    rand_num = np.random.rand()\n",
    "    train(epoch, rand_num, last_num)\n",
    "    last_num = rand_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(matrix, user_features, item_features, mask):\n",
    "    predicted_ratings = torch.sigmoid(torch.mm(user_features, item_features.t()))\n",
    "    pred = (predicted_ratings * (max_rating - min_rating) + min_rating) * mask\n",
    "    true_value = matrix * mask\n",
    "    \n",
    "    abs_error = torch.sum(torch.abs(pred - true_value))\n",
    "    square_error = torch.sum((pred - true_value)**2)\n",
    "    n_nonzero = torch.sum(mask)\n",
    "    return abs_error, square_error, n_nonzero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  0.69724214\n",
      "RMSE:  0.87678796\n"
     ]
    }
   ],
   "source": [
    "MAE = MSE = num_nonzero = 0\n",
    "\n",
    "for i in range(num_clients):\n",
    "    abs_error, square_error, n_nonzero = evaluate(test_rating_matrix[i*m: (i+1)*m], user_features[i], item_features[i], nonzero_mask[i*m: (i+1)*m])\n",
    "    MAE += abs_error\n",
    "    MSE += square_error\n",
    "    num_nonzero += n_nonzero\n",
    "\n",
    "MAE /= num_nonzero\n",
    "RMSE = torch.sqrt(MSE / num_nonzero)\n",
    "print(\"MAE: \", MAE.data.numpy())\n",
    "print(\"RMSE: \", RMSE.data.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ordinary",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
